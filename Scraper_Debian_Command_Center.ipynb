# 🚀 Updated Scraper Notebook with Manpage Integration
from google.colab import drive
import sqlite3
import requests
from datetime import datetime
import time

# 📂 Connect to Google Drive
drive.mount('/content/drive', force_remount=True)

# 📂 Define Database Path
db_path = "/content/drive/MyDrive/DebianCommandCenter/debian_commands.db"

# 🔗 URLs for Scraping
PACKAGE_LIST_URL = "https://packages.debian.org/experimental/allpackages?format=txt.gz"
MANPAGE_BASE_URL = "https://manpages.debian.org/bullseye"

# 🔄 Database Connection
def connect_to_db():
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    print(f"✅ Connected to database at: {db_path}")
    return conn, cursor

# ✅ Ensure Table Schema
def initialize_table(cursor):
    cursor.execute("""
    CREATE TABLE IF NOT EXISTS commands (
        name TEXT PRIMARY KEY,
        version TEXT,
        description TEXT,
        debports TEXT,
        manpage_url TEXT,
        date_created TEXT,
        last_updated TEXT
    )
    """)
    print("✅ Table 'commands' is initialized.")

# 📌 Fetch and Parse Package List
def scrape_package_list():
    print("🔄 Fetching package list...")
    response = requests.get(PACKAGE_LIST_URL, headers={"User-Agent": "Mozilla/5.0"})

    if response.status_code != 200:
        print(f"❌ Failed to retrieve package list. Status Code: {response.status_code}")
        return []

    content = response.text
    packages = []

    for line in content.splitlines():
        if not line.strip():
            continue

        try:
            name, rest = line.split(" ", 1)
            version = rest[rest.find("(") + 1:rest.find(")")]
            description = rest[rest.find(")") + 1:].strip()
            debports = None

            if "[" in description and "]" in description:
                debports = description[description.find("[") + 1:description.find("]")]
                description = description[:description.find("[")].strip()

            packages.append((name, version, description, debports))
        except ValueError:
            print(f"⚠️ Skipping malformed line: {line}")
            continue

    print(f"✅ Retrieved {len(packages)} packages.")
    return packages

# 📌 Get Manpage URL
def fetch_manpage_url(name):
    # Try to construct and validate the manpage URL
    manpage_url = f"{MANPAGE_BASE_URL}/{name}/en/"
    response = requests.get(manpage_url, headers={"User-Agent": "Mozilla/5.0"})
    if response.status_code == 200:
        print(f"✅ Manpage found for '{name}'")
        return manpage_url
    else:
        print(f"❌ No manpage found for '{name}'")
        return None

# 📌 Insert Data into Database
def insert_into_db(cursor, conn, name, version, description, debports):
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    manpage_url = fetch_manpage_url(name)

    cursor.execute("SELECT name FROM commands WHERE name=?", (name,))
    if cursor.fetchone():
        print(f"⚠️ Skipping '{name}'—already exists in database.")
    else:
        cursor.execute("INSERT INTO commands VALUES (?, ?, ?, ?, ?, ?, ?)", 
                       (name, version, description, debports, manpage_url, timestamp, timestamp))
        conn.commit()
        print(f"✅ Inserted '{name}'")

# 🎯 Run Scraper
def run_scraper():
    conn, cursor = connect_to_db()
    initialize_table(cursor)

    packages = scrape_package_list()

    if packages:
        print(f"📦 Found {len(packages)} packages to process...")
        for idx, package in enumerate(packages):
            name, version, description, debports = package
            print(f"🔄 Processing {idx + 1}/{len(packages)}: {name}")
            insert_into_db(cursor, conn, name, version, description, debports)
            # Optional: Add a small delay to prevent rate-limiting
            time.sleep(0.01)

        print("✅ Scraper completed successfully!")

    # Debugging: Verify Data
    cursor.execute("SELECT COUNT(*) FROM commands")
    row_count = cursor.fetchone()[0]
    print(f"📊 Total rows in 'commands' table: {row_count}")

    cursor.execute("SELECT * FROM commands LIMIT 5")
    rows = cursor.fetchall()
    print("📋 Sample rows:")
    for row in rows:
        print(row)

    conn.close()

# 🚀 Start Scraper
run_scraper()